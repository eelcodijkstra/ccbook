(ethiek-gevaren)=
# Gevaren van kunstmatige intelligentie

*Skynet, HAL 9000, V.I.K.I., the Matrix*, misschien ken je er zelf ook nog wel een paar: kwaadaardige kunstmatige intelligentie en robots. Het is al tijden een populair thema in sciencefictionboeken en -films. Natuurlijk zijn de situaties in deze films allemaal verzonnen, maar er zijn wetenschappers die echt vrezen dat denkende computers zich ooit tegen de mensheid zullen keren. Allereerst kun je je afvragen of computers wel intelligenter kunnen worden dan mensen, of dat computers √ºberhaupt intelligent kunnen zijn (daarover later meer). Toch blijft het een interessant onderwerp van debat en houden mensen zich ermee bezig. Het moment dat computers slimmer worden dan de mens wordt de **technologische singulariteit** genoemd. 

Is het eigenlijk wel logisch om bang te zijn voor superslimme computers? Misschien zorgen hyperintelligente computers in hun wijsheid juist wel voor vrede en harmonie. We weten het niet en misschien komen we er nooit achter.

Maar ook minder intelligente vormen van AI zouden kunnen worden ingezet om de mensheid te benadelen. Wat zal een AI-robot besluiten als je deze slechts √©√©n taak geeft en wel de volgende: zorg dat er geen overbodige CO‚ÇÇ wordt uitgestoten? De robot zou kunnen concluderen dat de gehele mensheid moet worden uitgeroeid.

## Misbruik van computers

Wat in elk geval w√©l een realistisch puntje van zorg is, is misbruik van kunstmatige intelligentie. Eigenlijk gebeurt dit nu al best vaak. Denk bijvoorbeeld aan deepfakes. Zo heeft [iemand](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/#5b10b1802241) een audio-deepfake ingezet om een bank voor $243.000 op te lichten.  Het is niet zozeer het computerprogramma zelf, maar de mens erachter die het gevaar vormt. Een ander voorbeeld van misbruik van kunstmatige intelligentie is misschien wel het gebruik van drones. Door drones volautomatisch te laten vliegen en ze bommen te laten werpen, kunnen mensen op een smerige manier  oorlog voeren. Kunstmatige intelligentie kan in de verkeerde handen als een extreem gevaarlijk wapen werken, iets wat we niet moeten willen!

## Computers die foutjes maken

‚ÄúComputers maken geen fouten, het zijn mensen die computers programmeren die fouten maken.‚Äù Je kent deze uitspraak misschien wel, en grotendeels klopt hij ook. Computerprogramma‚Äôs doen altijd precies waarvoor ze geprogrammeerd zijn. Toch ligt dit met kunstmatige intelligentie net iets anders. Bij kunstmatige intelligentie worden de acties van de software niet van tevoren geprogrammeerd, maar komt de software tot acties die deze zichzelf aanleert. In dit leerproces kunnen ongewenste fouten optreden. Het gevaar is hier dat mensen te veel gaan vertrouwen op kunstmatige intelligentie.

Een leuke anekdote gaat over een groep AI-robots op wieltjes, die moeten proberen elkaar zo min mogelijk te raken in een speelveld. De robots leren op een gegeven moment dat stilstaan de beste oplossing is. Geweldig bedacht, maar niet wat de ontwikkelaars in gedachten hadden. Een ander verrassend resultaat was de studie die [OpenAI](https://openai.com/blog/emergent-tool-use/) heeft gedaan naar AI in een "hide and seek" gamewereld. Na een tijd kwam de AI achter bugs in de software van de geprogrammeerde wereld, waardoor ze verder en hoger konden komen dan bedoeld.

Hiernaast vind je een artikel over een dodelijk ongeval door een ‚Äòzelfrijdende‚Äô auto. In het artikel staat een goed voorbeeld van een foutje van kunstmatige intelligentie met een treurig gevolg. Nu gaat het in dit voorbeeld over √©√©n slachtoffer, maar wat kan er allemaal niet misgaan als de straten straks overspoeld worden met zelfrijdende auto‚Äôs? Zelfs al zou een zelfrijdende auto minder fouten maken dan een mens, dan nog rest na een dergelijk ongeluk de vervelende vraag: wie is er verantwoordelijk voor de schade bij een ongeval? Is dat de eigenaar van de kunstmatig intelligente auto? Of is dat de programmeur van de software in de auto? Of misschien de kunstmatige intelligentie zelf?

[https://nos.nl/artikel/2114618-eerste-dode-door-zelfrijdende-auto.html](https://nos.nl/artikel/2114618-eerste-dode-door-zelfrijdende-auto.html)

:::{admonition} üõ†Ô∏è Vragen

1. Denk jij dat de technologische singulariteit realistisch is? Waarom wel/niet?
2. Stel dat computers in de toekomst inderdaad slimmer worden dan mensen. Denk jij dat dit dan goed of slecht voor de mensheid uitpakt? Leg uit waarom je dit denkt.
3. Welk soort gevaar van kunstmatige intelligentie vind jij het meest zorgelijk?
4. Wat zouden we kunnen doen om misbruik van kunstmatige intelligentie te voorkomen?

:::

:::{dropdown} Antwoorden

1. Dit hangt ervan af hoe je tegen intelligentie en tegen bewustzijn aankijkt. Als mensen eigenlijk ook een soort machines zijn, kunnen robots net zo goed bewustzijn krijgen. Maar als je voor bewustzijn iets nodig hebt wat niet te programmeren is (een ziel/geest of iets dergelijks), dan niet.
2. Dit ligt misschien aan de doelen die deze computers dan hebben. Als het doel is om te domineren, kan het slecht uitpakken. Als het doel is om in vrede voort te bestaan, zou het gunstig kunnen zijn.
3. Momenteel is de meest re√´le zorg dat mensen blindelings gaan vertrouwen op kunstmatige intelligentie. Dit ging bijvoorbeeld fout bij de zelfrijdende auto. Helaas wordt kunstmatige intelligentie ook gebruikt bij de ontwikkeling van wapens. Dit is ook een zorgelijke ontwikkeling.
4. Helemaal voorkomen is misschien niet mogelijk. Misbruik met wapens zou wellicht wel beperkt kunnen worden door het opstellen van verdragen. Landen die deze verdragen ondertekenen, nemen dan afstand van het gebruik van kunstmatige intelligentie met wapens.

:::

## Robots reguleren

We hebben net stilgestaan bij mogelijke gevaren uit de hoek van kunstmatige intelligentie. Als slimme computers echt deel gaan uitmaken van onze samenleving, zullen er dingen moeten veranderen. Er zullen nieuwe afspraken gemaakt moeten worden. Bijvoorbeeld over wie wanneer verantwoordelijk is bij een ongeluk met een robot. Of wat een slimme computer wel of niet mag doen.

Er wordt hierbij vaak verwezen naar Isaac Asimov en zijn drie wetten van de robotica. Hoewel Asimov deze wetten in een sciencefictionboek (*I, Robot*) publiceerde, is er vaak serieus over nagedacht. De wetten zijn bedoeld om mensen te beschermen tegen intelligente computers. Hieronder staan ze:

:::{admonition} Wetten van Asimov
:class: tip

1. Een robot mag een mens geen kwaad doen, of door niet te handelen toestaan dat een mens kwaad gedaan wordt.
2. Een robot moet de bevelen van mensen opvolgen, behalve als deze in strijd zijn met de eerste wet.
3. Een robot moet zijn eigen bestaan beschermen, zolang dit niet in conflict is met de eerste en de tweede wet.
:::

Asimov voegde later een ‚Äònulde‚Äô wet toe, die in feite gelijk is aan de eerste wet, maar waarin ‚Äúmens‚Äù is vervangen door ‚Äúmensheid‚Äù.

## Tekortkomingen van Asimovs wetten

Als er echt zoiets komt als de technologische singulariteit, is het goed om alvast na te denken over de vraag hoe we hiermee om moeten gaan. De bovengenoemde wetten zijn dan aardig om mee te beginnen. Er zijn echter ook tekortkomingen: problemen waar deze wetten geen rekening mee houden.

Als een robot bijvoorbeeld een eigen definitie van ‚Äòkwaad doen‚Äô verzint, kan de eerste wet een heel andere betekenis krijgen. Het kan ook zijn dat sommige robots niet eens weten of iets goed of kwaad is en daarmee de wetten overtreden. En wanneer is iets eigenlijk een robot? Geldt een mens met mechanische lichaamsdelen (zoals een robotarm) ook als een robot?

:::{admonition} üõ† Vragen

1. Wat vind jij van de wetten van Asimov? Zou je iets veranderen, toevoegen of weglaten?
2. Stel dat robots net zo slim worden als mensen. Vind je deze wetten dan nog wel eerlijk?
:::

:::{dropdown} Antwoorden

1. Een probleem met deze wetten is dat er ruimte voor eigen interpretatie is. Je kunt nog zelf invullen wat ‚Äòkwaad doen‚Äô precies betekent. Je zou wet 1 specifieker kunnen maken door op te sommen wat er wordt verstaan onder kwaad. Bijvoorbeeld: doden, stelen, kwetsen, enzovoorts.
2. Misschien zijn ze wel eerlijk. Mensen hebben de robots nu eenmaal gemaakt en hebben daarom het recht om wetten en regels in te stellen. Of misschien zijn ze niet eerlijk. Het is oneerlijk als een intelligente creatie de slaaf is van een andere intelligente creatie.

:::

## Rechten voor robots

We hebben het zojuist gehad over regels waaraan kunstmatige intelligentie en robots zich zouden moeten houden. Je kunt je vast goed voorstellen dat het belangrijk is dat er zulke regels komen. Wat mogelijk minder voor de hand ligt, is dat robots misschien ook wel rechten zouden moeten krijgen. Net zoals er voor dieren ook dierenrechten zijn. Zou het niet raar zijn als er straks ultra-intelligente robots rondlopen met minder rechten dan een kat?

Denk eens na, of discussieer eens met een klasgenoot, over de volgende vragen.

:::{admonition} üõ† Vragen

1. Mag je met een robot zomaar doen wat je wilt? Maakt het bij deze vraag uit hoe slim de robot is?
2. Als robots rechten zouden hebben, wat zouden deze dan volgens jou moeten zijn?
3. Zou een robot ooit minister-president mogen worden?
:::

:::{dropdown} Antwoorden

1. Je kunt beargumenteren dat een robot niet meer als product mag worden gebruikt wanneer deze bewustzijn heeft. Vergelijk het met slavernij: het houden van slaven is verkeerd, terwijl het houden van bijvoorbeeld werkpaarden wel geaccepteerd wordt. Een mens heeft een hoge mate van bewustzijn, een paard niet.  
Je kunt ook stellen dat een robot een machine blijft. Een machine is hoe dan ook een product, net zoals een tv, auto of balpen dat is. Je zou daarom volledig moeten kunnen beschikken over een robot.
2. Denk aan de rechten van de mens en de rechten van het dier. Je zou de rechten voor robots hieraan kunnen ontlenen. Misschien een combinatie van mensen- en dierenrechten?
3. Misschien is dit op dit moment ondenkbaar, maar wie weet wat de toekomst brengt. Een zwarte president was ooit ook ondenkbaar, net als een vrouw als regeringsleider. Normen en waarden veranderen door de tijd heen. Wellicht kijken we in de toekomst anders tegen robots aan en is het vanzelfsprekend dat ze dezelfde posities kunnen bekleden als mensen.

:::

Denk eens na over het volgende verhaal. Probeer dit zonder vooroordelen te lezen.

Je zit in een les informatica. Het is buiten mooi weer. De docent is bezig om iets uit te leggen over computerarchitectuur en je gedachten dwalen af, naar buiten. Langzaam maar zeker dagdroom je weg terwijl je uit het raam staart. Plotseling word je uit je dromerigheid getrokken als de docent roept: ‚ÄúH√© daar, wel op blijven letten.‚Äù Je doet meteen wat hij zegt. Natuurlijk, hij is de docent en jij de leerling, dus luister je.

Je bent aandachtig aan het luisteren als er ineens iets heel vreemds gebeurt. De docent valt midden in een zin stil en blijft compleet bewegingloos staan. De hele klas is in rep en roer en niemand merkt op dat de conci√´rge ondertussen het lokaal is binnengewandeld. Hij loopt rustig naar de nog altijd bewegingloze docent en draait wat aan zijn hoofd. Tot ieders verbazing draait hij de bovenkant van het hoofd van de docent als een soort deksel open. In het hoofd zitten allerlei radertjes en schroefjes: je docent blijkt een robot te zijn!

De conci√´rge sleutelt wat en draait het hoofd vervolgens weer dicht. Langzaam komt de docent weer op gang en hij gaat verder met zijn verhaal alsof er niets gebeurd is. De conci√´rge klopt de docent vriendelijk op de schouder en loopt rustig het lokaal uit. Iedereen is stomverbaasd.

Het mooie van filosofie is dat je overal over kunt en mag denken, hoe bizar iets ook is. Probeer je bovenstaand verhaal eens voor te stellen. Beantwoord daarna de volgende vragen.

:::{admonition} üõ† Vragen

1. Is er iets veranderd in de manier waarop je naar je docent kijkt, nu je weet dat hij een robot is? Wat is er veranderd en hoe komt dat?
2. Zou je nog steeds naar de docent luisteren? Waarom wel/niet?
3. Vind je dat deze robot-docent nog steeds dezelfde (mensen)rechten heeft?

:::

:::{dropdown} Antwoorden

1. Deze vragen zijn natuurlijk moeilijk, omdat de situatie ver van je af ligt. Ze doen een beroep op je fantasie. Je zult waarschijnlijk wel anders tegen de docent aankijken. De docent is niet wat je dacht dat hij was, en dat is schrikken.
2. Misschien heb je moeite met het idee dat een robot jou vertelt wat je moet doen. Toch is deze robot kennelijk door de schoolleiding aangesteld als jouw docent. Als je niet luistert, krijg je waarschijnlijk straf.
3. We hebben het al eens over robotrechten gehad. Misschien vind je dat de robot-docent nu ook robotrechten verdient. Of verandert er niets? Per slot van rekening gedraagt de docent zich nog zoals hij eerder ook deed.

:::

## De engheidsmatrix

Wanneer techneuten of social media influencers praten over AI, hebben ze het vaak over de ‚Äòcoole‚Äô, leuke en nuttige toepassingen daarvan. Vaak gaat het dan om de razend knappe nieuwe technieken die worden gebruikt, of over het allerlaatste, supersnelle neural network algoritme. Maar wanneer je sommige andere mensen over AI hoort, noemen zij AI juist vaak ‚Äòeng‚Äô. 

> ‚ÄúIk heb geen idee hoe het allemaal werkt, maar het is ons vast aan het bespioneren. Hartstikke eng, vind ik‚Ä¶‚Äù
> 

En eigenlijk hebben al deze mensen een beetje gelijk. Want AI kan gaaf en leuk zijn, maar het kan ook eng en fout zijn. In dit deel van de lessenserie ga je z√©lf aan de slag om te ontdekken wat voorbeelden zijn van goede, nuttige en leuke AI-toepassingen, en wat voorbeelden zijn van ‚Äòenge AI‚Äô.

Maar waarom vinden mensen de ene AI-toepassing nou ontzettend nuttig en ethisch verantwoord, en een andere juist heel eng en totaal overbodig? In paragraaf [1.4. In de praktijk](https://www.notion.so/1-4-In-de-praktijk-86f01e9c495f456cadb11a2ff7bba1cc) heb je een aantal voorbeelden gekregen waarin je hebt kennisgemaakt met de ethische aspecten van AI. Maar nu is het tijd te gaan bedenken wat jij z√©lf vindt van AI.

Een handige manier om zelf te beginnen met kritisch nadenken over AI is het maken van een **engheidsmatrix**. In zo'n matrix plot je verschillende AI-toepassingen om te ontdekken wat nou duidelijke voorbeelden van goede, nuttige en leuke AI-toepassingen zijn, en wat typisch voorbeelden zijn van ‚Äòenge AI‚Äô. Begin met het tekenen van een matrix, zoals in het voorbeeld hieronder. Het is handig om de matrix bijvoorbeeld groot op een muur of op een whiteboard te tekenen. Gebruik vervolgens een geeltje om een AI-toepassing te beschrijven en in de matrix te plaatsen, zodat je er met anderen over kunt discussi√´ren.

:::{figure} figs/ethiek-engheidsmatrix2.png
:width: 600

De engheidsmatrix
:::

:::{admonition} üõ† Groepsopdracht

1. Probeer eens de AI-toepassing [Quick, Draw!](https://quickdraw.withgoogle.com/) van Google in de matrix te plaatsen: dat is echt nog niet zo gemakkelijk. Hoe gaat Google met je gegevens om? Ga jij de Privacy Policy van Google doorspitten voordat je iets tekent? En hoe zit het met het nut van deze AI-toepassing?
2. Geef op het geeltje ook aan hoe je je keuzes maakt. Welke criteria gebruik je om *Quick, Draw!* op de horizontale as te plaatsen? Wanneer noem jij een AI-systeem eng, en wanneer vind je een systeem ethisch verantwoord? En welke criteria gebruik je om een AI-toepassing op de verticale as te plotten? Hoe bepaal je het nut van een AI-systeem? Onderbouw de keuzes die je maakt.
3. In paragraaf [1.4. In de praktijk](https://www.notion.so/1-4-In-de-praktijk-86f01e9c495f456cadb11a2ff7bba1cc) hebben wij bepaalde uitspraken bij de voorbeelden gedaan. Plaats jouw mening over deze voorbeelden in de matrix.
4. Vergelijk je keuzes met die van de andere leerlingen en discussieer over de uitkomsten. Waarin verschillen jullie uitkomsten? Kun je aangeven waarom die verschillen er zijn? En waar zijn jullie het eens?

Wil je meer weten over de engheidsmatrix en hoe je die kunt gebruiken om het nut en de ethische aspecten van een AI-toepassing in kaart te brengen? In het keuzedeel [E. Ethiek en AI](https://www.notion.so/E-Ethiek-en-AI-040d6779dbd947afa6ca13872c54245b)  gaan we verder op de matrix in.

:::