

<!DOCTYPE html>


<html lang="nl" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>5.2. N.2. Op weg naar meer lagen &#8212; Cognitive computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'neurale-netwerken/naar-meer-lagen';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Zoeken" href="../search.html" />
    <link rel="next" title="6. N.3 Tensorflow app" href="tensorflow-app.html" />
    <link rel="prev" title="5.1. N.1. Hoe leert een neuraal netwerk" href="hoe-leert-neuraal-netwerk.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="nl"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../materiaal.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Cognitive computing - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Cognitive computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Zoeken" aria-label="Zoeken" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Zoeken</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../materiaal.html">
                    Introductie
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Materiaal</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../achtergrond/achtergrond.html">1. Achtergrond</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../achtergrond/introductie.html">1.1. Introductie</a></li>
<li class="toctree-l2"><a class="reference internal" href="../achtergrond/geschiedenis.html">1.2. Geschiedenis en toekomst</a></li>
<li class="toctree-l2"><a class="reference internal" href="../achtergrond/intelligentie.html">1.3. Wanneer is iets intelligent?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../achtergrond/in-de-praktijk.html">1.4. In de praktijk</a></li>
<li class="toctree-l2"><a class="reference internal" href="../achtergrond/terugkoppeling.html">1.5. Terugkoppeling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ethiek/ethiek.html">2. Ethiek</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ethiek/gevaren.html">2.1. Gevaren van kunstmatige intelligentie</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek/fast-principes.html">2.2. De FAST-principes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek/fair-data.html">2.3. FAIR data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek/terugkoppeling.html">2.4. Terugkoppeling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../technieken/technieken.html">3. Technieken</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../technieken/introductie.html">3.1. Introductie</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technieken/machine-learning.html">3.2. Machine learning</a></li>

<li class="toctree-l2"><a class="reference internal" href="../technieken/associatie-analyse.html">3.4. Associatie-analyse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technieken/cluster-analyse.html">3.5. Clusteranalyse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technieken/neurale-netwerken.html">3.6. Neurale netwerken</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technieken/eigen-quickdraw.html">3.7. Eigen QuickDraw</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technieken/multi-agentsystemen.html">3.8. Multi-agentsystemen</a></li>

<li class="toctree-l2"><a class="reference internal" href="../technieken/terugkoppeling.html">3.10. Terugkoppeling</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Verdieping</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ethiek-en-ai/ethiek-en-ai.html">4. Ethiek en AI</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ethiek-en-ai/ai-standaarden.html">4.1. AI standaarden en richtlijnen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek-en-ai/fast-en-fair.html">4.2. FAST en FAIR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek-en-ai/engheidsmatrix.html">4.3. De engheidsmatrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek-en-ai/ethiek-opdracht-essay.html">4.4. Opdracht: schrijf een essay over Ethiek en AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ethiek-en-ai/ethiek-bedenk-een-ai.html">4.5. Opdracht: bedenk een AI toepassing</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="neurale-netwerken.html">5. Neurale netwerken</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hoe-leert-neuraal-netwerk.html">5.1. N.1. Hoe leert een neuraal netwerk</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.2. N.2. Op weg naar meer lagen</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tensorflow-app.html">6. N.3 Tensorflow app</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="docentenhandleiding.html">6.1. Docentenhandleiding</a></li>
<li class="toctree-l2"><a class="reference internal" href="fase-1.html">6.2. Fase 1: Maak een eerste app</a></li>
<li class="toctree-l2"><a class="reference internal" href="fase-2.html">6.3. Fase 2: Optimaliseren Neuraal Netwerk</a></li>
<li class="toctree-l2"><a class="reference internal" href="fase-3.html">6.4. Fase 3: Andere dataset gebruiken</a></li>
<li class="toctree-l2"><a class="reference internal" href="fase-4.html">6.5. Fase 4: Eigen dataset gebruiken</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../over-ons.html">Over ons</a></li>
<li class="toctree-l1"><a class="reference internal" href="../colofon.html">Colofon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../achtergrond/robotica.html">Robotica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/verzamelingen.html">Verzamelingenleer</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/keuzethemas/ccbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Bronopslagplaats"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/keuzethemas/ccbook/issues/new?title=Issue%20on%20page%20%2Fneurale-netwerken/naar-meer-lagen.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open een probleem"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download deze pagina">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/neurale-netwerken/naar-meer-lagen.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download het bronbestand"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Afdrukken naar pdf"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Volledig scherm"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Zoeken" aria-label="Zoeken" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>N.2. Op weg naar meer lagen</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Inhoud </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simpele-logische-schakelingen">5.2.1. Simpele logische schakelingen</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dierenliefhebber">5.2.1.1. Dierenliefhebber</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hotel-de-botel-van-dieren">5.2.1.2. Hotel de botel van dieren</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verantwoordelijke-voortplanting">5.2.1.3. Verantwoordelijke voortplanting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">5.2.2. Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#veel-gegevens-toch-simpel-lijnen">5.2.3. Veel gegevens toch simpel: lijnen</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lijn-y-2x-1-ofwel-x-frac-1-2-y-frac-1-2">5.2.3.1. Lijn <span class="math notranslate nohighlight">\(y=2x-1\)</span> ofwel <span class="math notranslate nohighlight">\(x - \frac{1}{2}y= \frac{1}{2}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lijn-y-2x1-ofwel-x-1-frac-1-2-x-2-frac-1-2-met-fouten-in-de-trainingsdata">5.2.3.2. Lijn <span class="math notranslate nohighlight">\(y=2x−1\)</span> ofwel <span class="math notranslate nohighlight">\(x_{1} -\frac{1}{2}x_{2}=\frac{1}{2}\)</span> met fouten in de trainingsdata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-vervolgd">5.2.4. Bias vervolgd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lijnen-vervolgd">5.2.5. Lijnen vervolgd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigen-quickdraw-vervolgd">5.2.6. Eigen Quickdraw vervolgd</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="n-2-op-weg-naar-meer-lagen">
<h1><span class="section-number">5.2. </span>N.2. Op weg naar meer lagen<a class="headerlink" href="#n-2-op-weg-naar-meer-lagen" title="Permalink to this heading">#</a></h1>
<p>In het hoofdstuk <a class="reference external" href="https://www.notion.so/3-5-Neurale-netwerken-ae6558ef79354551bb5442641452babb">technieken</a> zijn al verschillende netwerken gepresenteerd en welke leerstrategieën er zijn. Ook hebben we in de <a class="reference external" href="https://www.notion.so/N-1-Hoe-leert-een-neuraal-netwerk-f6445c4d563a48dd867c16d63fc55254">vorige paragraaf</a> het kleinste netwerk, de preceptron, in meer detail bekeken en deze via <em>begeleid leren</em> getraind tot een zo klein mogelijke fout in de voorspelling. Je hebt daar gezien dat de gewichten van de enige perceptron in dit netwerk zich ook bewijsbaar laat aanpassen aan de trainingsdata, door in te spelen op de grootte van de fout. De gebruikte trainingsmethode valt onder wat men noemt de <em>gradiënt technieken</em>.</p>
<p><a class="reference external" href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d1c097ca-cdcc-4c5c-af1e-bebee48b7804/downwards.jfif">downwards.jfif</a></p>
<p><img alt="Naarhetlaagstepunt.gif" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6e6fd8a2-ef80-4293-8cdc-a05ad2d27420/Naarhetlaagstepunt.gif" /></p>
<p><img alt="figuur 1: Een fouten landschap en het pad naar de kleinste fout met een gradiënt techniek. Volg de steilste helling vanaf het startpunt. Verschillende startpunten kunnen een ander minimum opleveren." src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/978c9efd-3696-4ca2-8f40-766deb2a61bb/Naarhetlaagstepunt2.png" /></p>
<p>figuur 1: Een fouten landschap en het pad naar de kleinste fout met een gradiënt techniek. Volg de steilste helling vanaf het startpunt. Verschillende startpunten kunnen een ander minimum opleveren.</p>
<p>Bij een gradiënt techniek wordt in een toestand bepaald in welke richting de toestand het snelst kan veranderen in de richting van een gewenste toestand. In de wiskunde wordt snelheid in een punt (=toestand) bepaald door de waarde van de afgeleide (=helling = gradiënt) in dat punt. In het trainen van neurale netwerken is de gewenste toestand het punt waar de fout zo klein mogelijk is. In figuur 1 zie je een hypothetisch voorbeeld. Bij een gegeven set trainingsdata zie je een berglandschap met op de z-as de grootte van de fout bij bepaalde waarden van de gewichten (de twee andere assen, dus hier gaat het slechts om twee gewichten). Bij het leren start je ergens in dit landschap en volg je het steilste pad naar een laag punt in het landschap. De zwarte lijntjes zijn voorbeelden van zulke paden in dit landschap. Je ziet dat je niet altijd op de juiste of wel de laagste plek in het landschap terecht komt. Het startpunt en de vorm van het landschap bepaalt waar je uiteindelijk terecht komt. Er wordt dan ook vaak meerdere keren vanuit andere startpunten geleerd. In de meeste neurale netwerken zijn er veel knopen en meerdere lagen, dus ook veel meer gewichten (is meer assen en dus nog meer dimensies) die aangepast moeten worden. Het foutenlandschap kan dan nog ingewikkelder worden.</p>
<p>Als er meerdere verborgen lagen in het netwerk aanwezig zijn, in de meeste gevallen dus, is het idee van trainen hetzelfde als bij het netwerk bestaande uit één perceptron. Het probleem is dat je de fout bij een trainingspunt alleen meet aan het eind van het netwerk. Echter ook in de tussenlaag hebben we een fout nodig om te kunnen corrigeren. De techniek om de uiteindelijke fout door het netwerk te sturen noemt men <strong>back propagation</strong>. In de onderstaande video wordt dit idee uitgelegd en in een iets bredere context geplaatst.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=FaHHWdsIYQg">https://www.youtube.com/watch?v=FaHHWdsIYQg</a></p>
<p>In deze cursus gaan we niet op de wiskunde van back propagation in, omdat, zoals je in de video kan zien, de onderliggende wiskundige technieken nog niet bij jullie bekend zijn. Voel je je echter uitgedaagd dan zijn er vele plekken met uitleg op het internet te vinden, b.v. <a class="reference external" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">Matt Mazu</a>r en <a class="reference external" href="https://youtu.be/8d6jf7s6_Qs">Mikael Laine</a>.</p>
<p>In dit hoofdstuk ga je onderzoeken wat het effect is van keuzes in vorm van het netwerk en de keuzes in de instellingen van het trainingsproces. Je gebruikt daarvoor een aantal applets die zijn gebouwd op de bibliotheek van <a class="reference external" href="https://brain.js.org/#/">brain.js</a>. In deze applets bieden we je een aantal voorbeelden aan, maar kun je zo je wilt ook eigen trainingsdata voor een probleem invoeren. Al deze voorbeelden blijven eenvoudig. Het doel is te ervaren waar je zoal op moet letten als je een netwerk bouwt en gaat trainen.</p>
<section id="simpele-logische-schakelingen">
<h2><span class="section-number">5.2.1. </span>Simpele logische schakelingen<a class="headerlink" href="#simpele-logische-schakelingen" title="Permalink to this heading">#</a></h2>
<p>We beginnen met het trainen van een simpel netwerk dat moet leren om te bepalen of iets waar is of niet waar, gebaseerd op een netwerk met twee inputs die elk waar of niet waar zijn. Om dit een beetje een context te geven gaan we dit voorbeeld ophangen aan de huisdieren kat en hond en kindertal. We laten het netwerk dan een uitspraak doen over jouw keuze.</p>
<p><img alt="training.png" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3eb6ca9a-01ac-4566-b5ff-33ecec997e07/training.png" /></p>
<section id="dierenliefhebber">
<h3><span class="section-number">5.2.1.1. </span>Dierenliefhebber<a class="headerlink" href="#dierenliefhebber" title="Permalink to this heading">#</a></h3>
<p>Een persoon kan een kat, een hond, beide of geen van beide als huisdier hebben. Heeft de persoon er minstens één, dan vinden we het een dierenliefhebber, anders niet. Zie de tabel links in figuur 2. Dit is een voorbeeld van de logische schakeling of ofwel de <strong>of</strong>-poort. We kunnen dit voorbeeld omzetten in getallen zodat we het de computer kunnen voeren, nee wordt 0 en ja wordt 1. Het wel of niet hebben van een kat wordt de eerste input (x1) en het wel of niet hebben van een hond wordt de tweede input (x2) van het netwerk. Een abstractere weergave van dit probleem wordt dan de tabel rechts. Deze tabel geeft alle combinaties weer die mogelijk voor dit probleem en daarmee gaan we ons netwerk trainen. Aan het eind van het trainen verwacht je dat  de invoer [1,0] de uitvoer 1 levert, ofwel in het bezit van een kat ben je een dierenliefhebber.</p>
<p><img alt="figuur 2: een voorbeeld voor de logische schakeling of." src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e42bcb39-c8ca-4fbe-99f8-50426b255b4e/or_dier.svg" /></p>
<p>figuur 2: een voorbeeld voor de logische schakeling of.</p>
<p>Voor dit probleem een neuraal netwerk gebruiken is echte onzin, want je kunt zonder trainen jouw vraag opzoeken in de tabel. Dat we het toch doen is om te leren hoe effectief het netwerk kan leren. Open deze <a class="reference external" href="https://www.johnval.nl/school/informatica/Artificial_Intelligence/SLO-apps/NNinputAll.htm">applet</a> en ga op onderzoek uit. De activeringsfunctie die in dit voorbeeld wordt gebruikt is de sigmoïde functie <span class="math notranslate nohighlight">\(\sigma(x)\)</span>, die waarden tussen 0 en 1 kan aannemen, echter niet de waarden 0 en 1 zelf. Hoe groter (of kleiner) <span class="math notranslate nohighlight">\(x\)</span> hoe dichter  <span class="math notranslate nohighlight">\(\sigma(x)\)</span> bij 1 ( of 0)</p>
<p><img alt="activeringsfuncties.png" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c41cabc0-d954-45f0-b36c-79fb1c7853c7/activeringsfuncties.png" /></p>
<ul>
<li><p>Opdrachten</p>
<ol class="arabic">
<li><p>Druk vier keer op de Train knop en vul de tabel in van opdracht 1 in aanwezig in onderstaand Excel document en beantwoord de volgende vragen:</p>
<p><a class="reference external" href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/154e8764-d7ef-443a-8d81-ecc99e68f2d4/OpdrachtenNN2.xlsx">OpdrachtenNN2.xlsx</a></p>
<ol class="arabic simple">
<li><p>Is het aantal iteraties altijd gelijk?</p></li>
<li><p>Is de fout altijd gelijk?</p></li>
<li><p>Zijn de vergelijkingen precies gelijk?</p></li>
<li><p>Wat is hiervoor de verklaring?</p></li>
</ol>
<ul>
<li><p>Antwoord</p>
<p>Bij het begin van iedere training worden de de gewichten in de vergelijkingen bij toeval gekozen. Dit zorgt ervoor dat het pad naar de uiteindelijke oplossing niet gelijk is (zoiets als een andere startwaarde in figuur 1) en de grens die je aan de fout hebt gesteld met een net andere waarde wordt overschreden. Als je naar de geschaalde vergelijkingen kijkt zie je wel bijna dezelfde vergelijking.</p>
</li>
</ul>
</li>
<li><p>Waarom is het trainen zonder verborgen lagen hier mogelijk?</p>
<ul>
<li><p>Antwoord</p>
<p>Je ziet in de grafische weergave dat er in de puntenwolk één lijn kan worden getekend die de rode en de blauwe punten van elkaar kan scheiden. Dit is precies de situatie die een enkele perceptron aankan. In het <strong>or</strong> netwerk is dat de knoop in de output laag.</p>
</li>
</ul>
</li>
<li><p>In deze opgave bekijken we het effect van de grenswaarde  in de fout (treshold). Vul de tabel bij opdracht 3 in bovenstaand Excel document en beantwoord de volgende vragen:</p>
<ol class="arabic">
<li><p>Bij groter wordende fout neemt het aantal iteraties af. Welke verband zie je verschijnen, lineair, exponetiëel, omgekeerd evenredig, kwadratisch (maak een grafiek met de getallen)?</p>
<ul>
<li><p>Antwoord</p>
<p>Ons resultaat. Maak zelf de grafiek en zie een hyperbolisch verband verschijnen.</p>
<p>Maak zelf de grafiek en zie een omgekeerd evenredig (hyperbolisch) verband verschijnen, <span class="math notranslate nohighlight">\(iteraties = \frac{c}{fout}\)</span>.</p>
<p><img alt="Or_fout_iteraties.png" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2d267ce4-1518-4873-908a-3c0146f8bfa3/Or_fout_iteraties.png" /></p>
</li>
</ul>
</li>
<li><p>Kijk naar de geschaalde vergelijkingen ( <span class="math notranslate nohighlight">\(x_1+ax_2=b\)</span> ) Wat valt je op?</p>
<ul>
<li><p>Antwoord</p>
<p>Op de constante b na lijken de lijnen bijna gelijk.</p>
</li>
</ul>
</li>
<li><p>De geschaalde vergelijkingen zijn verkregen door te delen door de factor voor <span class="math notranslate nohighlight">\(x_1\)</span> in de ongeschaalde vergelijkingen. Als je een invoer (b.v. <span class="math notranslate nohighlight">\([x_1,x_2]=[0,0]\)</span>) aan het netwerk geeft berekent het netwerk met deze invoer de som:
<span class="math notranslate nohighlight">\(som=a \cdot x_1+b\cdot x_2 + c\)</span>
Vul nu de laatste twee kolommen voor invoer [0,0] in (vergeet niet bij de geschaalde vergelijkingen de constante term naar links te halen).Vul de waarden gevonden in deze kolommen in, in de functie <span class="math notranslate nohighlight">\(\sigma(som) = sigmoïde(som) = 1/(1+e^{-som})\)</span>.
Wat valt je op als je de fout kleiner maakt?</p>
<ul>
<li><p>Antwoord</p>
<p>Invullen van [0,0] in de ongeschaalde som levert precies de voorspelde waarde van het netwerk. Bij kleinere fout verandert de positie van de lijn nauwelijks. De waarden van de coëfficiënten nemen toe waardoor de som groter wordt en beweeg je verder naar de extremen van de sigmoïde functie. Dit gebeurt ook in de knopen van netwerken met meer lagen. De invoer naar opvolgende knopen zorgen voor een hoger onderscheidend vermogen van het netwerk als die invoer grotere verschillen heeft. b.v. (0,1) versus (0.45 , 0.55). Dus grotere gewichten leiden tot een groter onderscheidend vermogen.</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</section>
<section id="hotel-de-botel-van-dieren">
<h3><span class="section-number">5.2.1.2. </span>Hotel de botel van dieren<a class="headerlink" href="#hotel-de-botel-van-dieren" title="Permalink to this heading">#</a></h3>
<p>Weer kan een persoon kan een kat, een hond, beide of geen van beide als huisdier hebben. Heeft de persoon er twee, dan is deze persoon hotel de botel ofwel stapelgek op dieren, anders niet. Zie de tabel links in figuur 3. Dit is een voorbeeld van de logische schakeling <strong>and</strong> ofwel de en-poort. De abstractere weergave van dit probleem wordt nu de tabel rechts. Open weer de <a class="reference external" href="https://www.johnval.nl/school/informatica/Artificial_Intelligence/SLO-apps/NNinputAll.htm">applet</a> en ga op onderzoek uit.</p>
<p><img alt="figuur 3: voorbeeld voor de logische schakeling en." src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d68dad32-c432-4341-a07c-0addd8449cfa/and_dier.svg" /></p>
<p>figuur 3: voorbeeld voor de logische schakeling en.</p>
<ul>
<li><p>Opdracht</p>
<ol class="arabic">
<li><p>Verander de trainingsdata naar de uitvoer van de en-poort gegeven in de tabel rechts:
Wat observeer je als je het netwerk dan traint?</p>
<ul>
<li><p>Antwoord</p>
<p>Je observeert dat er ook hier een lijn is die net als bij de or-poort de punten netjes verdeeld. De belangrijkste observatie is dat door alleen de trainingsdata te veranderen en niet het programma te wijzigen het netwerk ook andere situaties kan leren. Ofwel de trainingsdata bepaalt het voorspellend vermogen van het netwerk.</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="verantwoordelijke-voortplanting">
<h3><span class="section-number">5.2.1.3. </span>Verantwoordelijke voortplanting<a class="headerlink" href="#verantwoordelijke-voortplanting" title="Permalink to this heading">#</a></h3>
<p>Op dit moment zijn er meer dan 7 miljard mensen op aarde, deze hebben niet allemaal dezelfde welvaart. Als iedere wereldburger dezelfde welvaart als de gemiddelde Nederlander zou hebben dan is, volgens sommigen, de draagkracht van de aarde 1,5 tot 2 miljard mensen. Om terug te gaan naar deze bevolkingsomvang zouden gezinnen een tijd precies hooguit één nakomeling moeten krijgen. Echter een mens wil zijn genen ook graag doorgeven naar een volgende generatie. Dus één kind per stel is dan nodig. Laten we ervan uitgaan dat we in de toekomst het geslacht van kind, een jongen of een meisje, zouden kunnen kiezen en dat we er van ieder geslacht maximaal één kind kunnen krijgen dan ben je als stel verantwoordelijk als je één kind krijgt. Deze situatie is in de tabel links weergegeven en in de tabel rechts omgezet naar een getalswaarde. Deze situatie noemt men de <strong>exlusieve-of poort</strong>. (xor (⊻)).</p>
<p><img alt="figuur 4: voorbeeld van een xor schakeling." src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/78565b98-f7b0-4c90-a886-f404a23fcb72/xor_voortplanting.svg" /></p>
<p>figuur 4: voorbeeld van een xor schakeling.</p>
<ul>
<li><p>Opdrachten</p>
<ol class="arabic">
<li><p>Verander de trainingsdata naar de uitvoer van de exclusieve of-poort gegeven in de tabel uit figuur 4: Je kunt dit netwerk niet trainen met de zelfde instellingen als bij de en-poort en de of-poort. Waarom niet? Wat moet er veranderen?</p>
<ul>
<li><p>Antwoord</p>
<p>Je ziet dat het niet mogelijk is om de puntenwolk met slechts
één lijn te scheiden. Met één enkele perceptron kan er slechts
één lijn worden gemaakt. We hebben meerdere lagen nodig om
dit probleem te trainen. Laad in de applet het xor voorbeeld door op de knop xor te drukken en beantwoordt de volgende vragen.</p>
</li>
</ul>
</li>
<li><p>Het kan zijn dat bij het indrukken van de <strong>xor</strong> knop in het menu, de training al direct mislukt. Er is bewust een situatie gecreëerd die vaak een slecht trainingsresultaat geeft.  In het Excel document in het tabblad “Logische schakeling xor” vul je onder opdracht 6 de tabel in door per situatie  50 keer op de <strong>Train</strong> knop te drukken en dan het aantal trainingen en het aantal mislukkingen te tellen.
Beantwoord daarna de volgende vragen:</p>
<ol class="arabic">
<li><p>Is het aantal mislukkingen bij iedere leersnelheid gelijk?</p>
<ul>
<li><p>Antwoord</p>
<p>Iedere keer dat je op de train knop drukt worden de gewichten <span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span> en <span class="math notranslate nohighlight">\(w_2\)</span>
bij toeval gekozen. Sommige combinaties blijken tot een niet succesvolle training te leiden. De kans dat het misgaat blijkt in onze simulaties groter te zijn als de leersnelheid hoger is. Echter bij een leersnelheid van 0.1 vonden wij ook nog
een flink aantal mislukkingen. Onze reeks van 50 trainingen leverde (0.9:13, 0.7:8, 0.5:9, 0.3:4, 0.1:5)</p>
</li>
</ul>
</li>
<li><p>Waarom moet je het maximaal aantal iteraties verhogen bij lagere leersnelheden?</p>
<ul>
<li><p>Antwoord</p>
<p>Lagere leersnelheden zorgen voor kleinere stapjes in de ruimte van mogelijke waarden voor de gewichten. Als een bepaalde set van gewichten moet worden bereikt dan doet een mier daar langer over dan een luipaard. Maar zoals we  hierboven hebben ontdekt kan een luipaard zijn doel voorbij schieten.</p>
</li>
</ul>
</li>
<li><p>Wat valt je op in het diagram bij een mislukte training?</p>
<ul>
<li><p>Antwoord</p>
<p>Er is minstens één van de twee lijnen die niet op de juiste manier de punten verdeeld.</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>In vorige opdracht zag je dat het leren zelfs bij kleine leersnelheden niet altijd tot een succesvolle training leidde. We gaan nu onderzoeken of meer knopen in een laag daar verbetering in kan brengen. Zet de leersnelheid op 0.9. Herhaal weer de procedure als boven, maar noteer ook hoe veel lijnen in het diagram de punten onjuist verdelen, ofwel een rode en blauwe regio juist scheiden.</p>
<ol class="arabic">
<li><p>Is er altijd een succesvolle training?</p>
<ul>
<li><p>Antwoord</p>
<p>In onze poging met met 3 knopen [3] trad er slechts één mislukking op
in de 50 trainingen. In dit geval waren er 2 lijnen die
de puntenwolk niet juist verdeelden. In 20 van de trainingen deelden alle 3 de lijnen de punten wolk op een juiste manier.
Bij 4 knopen trad er in 200 keer trainen geen enkele mislukking op en deelden in slechts 5 gevallen alle
lijnen de punten op een juiste manier in twee groepen.</p>
</li>
</ul>
</li>
<li><p>Wat zou een verklaring kunnen zijn voor het succes van meerdere knopen in de verborgen laag?</p>
<ul>
<li><p>Antwoord</p>
<p>Het lijkt er op dat er minimaal twee lijnen juist moeten liggen. Met meer knopen is de kans daarop groter. Het netwerk kan een slechte set start gewichten bij één knoop blijkbaar compenseren met de andere knopen. Dus meer knopen geven meer succes.</p>
<p><em>Voor degenen die iets van kansrekening weten zou een benadering met de binomiale verdeling misschien inzicht kunnen geven:
Bij 2 knopen en threshold 0.9 vonden wij 2 juiste lijnen in 37 pogingen bij 50 herhalingen ofwel P=37/50=0.74.
Omdat P( precies 2 keer een succes,n=2) = <span class="math notranslate nohighlight">\(p^2\)</span> is de kans p op eén juiste lijn = <span class="math notranslate nohighlight">\(\sqrt{0.74}=0.86\)</span>.
Zetten we deze kans door naar meer lagen dan levert de binomiale verdeling:
Bij 3 knopen P(minstens 2 successen, n=3,p=0.86)=0.95.
Bij 4 knopen p(minstens 2 successen, n=4,p=0.86)=0.99.
In 50 herhalingen verwacht je dan 50⋅0.95=47.550⋅0.95=47.550⋅0.95=47.550⋅0.95=47.5﻿ successen bij 3 knopen en
50⋅0.99=49.550⋅0.99=49.550⋅0.99=49.550⋅0.99=49.5﻿ successen bij 4 knopen.</em></p>
</li>
</ul>
</li>
<li><p>Hoeveel iteraties zijn er nodig bij een succesvolle training bij 2 , 3 en 4 knopen bij een leersnelheid van 0.9?</p>
<ul>
<li><p>Antwoord</p>
<p>Wij observeerden in bij 3 en 4 knopen gevallen rond de 1000 iteraties.
Bij 2 knopen waren er vaak uitschieters naar veel meer dan 1000 iteraties nodig.</p>
</li>
</ul>
</li>
<li><p>Welke strategie lijkt meer succes te leveren, meer knopen per laag of een lagere leersnelheid?</p>
<ul>
<li><p>Antwoord</p>
<p>Meer knopen in een tussenlaag levert met een hogere leersnelheid vaker tot een succesvolle training met minder iteraties.</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</section>
</section>
<section id="bias">
<h2><span class="section-number">5.2.2. </span>Bias<a class="headerlink" href="#bias" title="Permalink to this heading">#</a></h2>
<p><img alt="figuur 5: De FAST-principes" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5b6e4e83-e414-4a34-998c-0efd2f554c82/FAST.png" /></p>
<p>figuur 5: De FAST-principes</p>
<p>Bij het maken van een applicatie die gebruikt maakt van neurale netwerken, of AI in het algemeen, is het van groot belang aandacht te besteden aan de mogelijke problemen die deze applicatie met zich draagt. In deze cursus hebben we daarom de sectie ethiek een grote plek toebedeeld. De <strong>FAST-principes</strong> ( fairness, accountability, safety en transparency ) worden daarin uitgelegd. In dit hoofdstuk zijn we neurale netwerken aan het trainen. De opdrachten zijn er om te ontdekken hoe een netwerk te trainen is door verschillende keuzes te maken. In figuur 5 zie je onder <strong>Fairness</strong> de term <strong>bias</strong> staan en onder <strong>Transparency</strong> de termen <strong>Rechtvaardiging</strong> en <strong>Verklaarbaarheid</strong>. We beginnen met rechtvaardiging en verklaarbaarheid van een AI applicatie, gebaseerd op neurale netwerken. Om aan deze principes te kunnen voldoen moet er onder andere de volgende punten in de documentatie aanwezig zijn:</p>
<ol class="arabic simple">
<li><p>de keuze in het ontwerp (design) van het netwerk</p></li>
<li><p>de keuze in de implementatie van het netwerk</p></li>
<li><p>de eigenschappen van de trainingsdata gebruikt om het netwerk te trainen</p></li>
<li><p>een onderbouwing waarom de uitkomst bij de trainingsdata op waarheid berust</p></li>
</ol>
<p>Al deze punten vallen onder de term bias.</p>
<p>De Engelse term bias is een <strong>homoniem</strong>, ofwel één woord met meerdere betekenissen. Het lastige is dat deze verschillende betekenissen op verschillende plaatsen in het ontwikkeltraject van een AI applicatie effect hebben op de <strong>fairness</strong> van die applicatie.
De voor AI belangrijke betekenissen van bias zijn:</p>
<ol class="arabic simple">
<li><p>bias = vooringenomenheid</p></li>
<li><p>bias = vooroordeel</p></li>
<li><p>bias = neiging</p></li>
<li><p>bias = effect</p></li>
</ol>
<p>Al deze betekenissen hebben effect op het design, trainingsdata en training van het netwerk. De eerste twee betekenissen zijn eigenlijk gelijk en zijn van belang in het voortraject van het bouwen van een applicatie. De derde en de vierde betekenis zijn van belang in de beschouwing van de kwaliteit van de trainingsdata.</p>
<ul>
<li><p>Maak nu deze belangrijke opdracht</p>
<ol class="arabic simple">
<li><p>Is er sprake van bias in de “applicaties” die we hebben gemaakt voor de logische schakelingen?</p></li>
</ol>
<ul>
<li><p>Antwoord</p>
<p>Wel degelijk! Één van de auteurs wilde heel graag wat voorbeelden bij elk van de logische schakelingen. De voorbeelden staan stijf van de vooroordelen:</p>
<ul class="simple">
<li><p>In al deze voorbeelden hebben we slechts twee inputs toegestaan: geen andere huisdieren, geen mogelijkheid voor twee jongens, twee meisjes of gender neutrale personen.</p></li>
<li><p>De beperking in het aantal mogelijke inputs zorgt er voor dat je de applicatie geen voorspelling kan laten doen over situaties die niet in de trainingsdata voorkomen.
Dit is een vooroordeel over de mogelijke situaties die we toestaan en die doorwerkt in de bruikbaarheid van een applicatie. Mee te nemen in het traject <em>Rechtvaardiging en verklaarbaarheid.</em></p></li>
<li><p>Voor de combinaties van input hebben we een uitspraak gedaan over de uitkomst: Wel of niet een dierenliefhebber, wel of niet stapelgek op dieren, wel of niet een verantwoordelijke wereldburger. Deze uitkomst hebben de makers van de applicatie opgelegd als uitkomst van de applicatie en kan een vooringenomenheid zijn.
<strong>Als er niet eerst gedegen onderzoek wordt gedaan naar de waarheid van deze uitkomsten bij de gegeven input dan kan een AI applicatie de reinste onzin produceren. Oorzaak-gevolg relaties worden heel vaak fout gelegd en het is dus
zaak te documenteren waar de gebruikte oorzaak-gevolg relatie vandaan komt.</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In de opdracht hierboven heb je geleerd dat de aannames die je maakt van groot belang zijn voor de AI applicatie die je maakt. De voorbeelden bij de logische schakelingen zijn heel erg vooringenomen. Er is echter een absolute noodzaak tot vooringenomenheid voor AI applicaties. Een AI applicatie moet voorspellingen kunnen doen, ook in situaties die de applicatie niet eerder heeft gezien. De trainingsdata vormen een landschap. De AI applicatie leert dit landschap op basis van de trainingsdata. Als de applicatie vervolgens een uitspraak moet doen voor een datapunt dat niet gelijk is aan één van de trainingspunten dan gaat de ontwikkelaar van de applicatie er van uit dat dit datapunt en de geleverde voorspelling netjes in dit landschap past.</p>
<p>Op de twee andere betekenissen van bias <strong>neiging</strong> en <strong>effect</strong> gaan we dieper in nadat jullie eerst weer wat onderzoek hebben gedaan aan netwerken die moeten leren gebieden te herkennen.</p>
</section>
<section id="veel-gegevens-toch-simpel-lijnen">
<h2><span class="section-number">5.2.3. </span>Veel gegevens toch simpel: lijnen<a class="headerlink" href="#veel-gegevens-toch-simpel-lijnen" title="Permalink to this heading">#</a></h2>
<section id="lijn-y-2x-1-ofwel-x-frac-1-2-y-frac-1-2">
<h3><span class="section-number">5.2.3.1. </span>Lijn <span class="math notranslate nohighlight">\(y=2x-1\)</span> ofwel <span class="math notranslate nohighlight">\(x - \frac{1}{2}y= \frac{1}{2}\)</span><a class="headerlink" href="#lijn-y-2x-1-ofwel-x-frac-1-2-y-frac-1-2" title="Permalink to this heading">#</a></h3>
<p>In de paragraaf <a class="reference external" href="https://www.notion.so/N-1-Hoe-leert-een-neuraal-netwerk-f6445c4d563a48dd867c16d63fc55254">Hoe leert een neuraal netwerk</a> hebben we het leren van een perceptron uitgelegd met behulp van punten die boven (uitvoer = 1) dan wel onder (uitvoer = -1) de lijn y=2x-1 liggen. Deze lijn zou bijvoorbeeld een scheidslijn kunnen zijn van twee ondergrondse aardlagen. Om deze grens te bepalen boort men willekeurig gaten in de grond en bepaald men het type van de aardlaag.</p>
<p>In dit voorbeeld en de andere voorbeelden met lijnen bekijken we de invloed van de datapunten op de training van netwerk. De activeringsfunctie die in dit voorbeeld wordt gebruikt is de tangenshyperbolicus functie <span class="math notranslate nohighlight">\(tanh(x)\)</span>, die waarden tussen -1 en 1 kan aannemen, echter niet de waarden -1 en 1 zelf. Hoe groter (of kleiner) <span class="math notranslate nohighlight">\(x\)</span> hoe dichter  <span class="math notranslate nohighlight">\(\tanh(x)\)</span> bij 1 ( of -1)</p>
<ul>
<li><p>Opdrachten</p>
<p>In deze opdrachten maken we weer gebruik van de <a class="reference external" href="https://www.johnval.nl/school/informatica/Artificial_Intelligence/SLO-apps/NNinputAll.htm">applet</a> maar nu kies je de optie lijn. Het simpele perceptron netwerk maakt het mogelijk een lijn die de punten scheidt te vinden. Deze lijn valt, afhankelijk van de set trainingspunten, meer of minder samen met de lijn <span class="math notranslate nohighlight">\(y=2x-1\)</span>. In de vorige paragraaf gebruikten we de teken functie als activeringsfunctie in het trainingsproces. Brain.js heeft deze activeringsfunctie niet. De tangens hyperbolicus functie die ook in de vorige paragraaf is gepresenteerd gebruiken we als alternatief. De tangens hyperbolicus kan ook in netwerken met meer lagen worden gebruikt, waar een 1 of -1 situatie als uitvoer nodig is.</p>
<p><img alt="Nogmaals de activeringsfuncties" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8ac9871f-11ae-4d85-8d6b-c77c0810cafc/activeringsfuncties.png" /></p>
<p>Nogmaals de activeringsfuncties</p>
<p>De app in deze pagina kan ook hoger dimensionale netwerken aan (= knopen met meer dan twee elementen in de input lijst). De lijn  <span class="math notranslate nohighlight">\(y=2x−1\)</span> wordt in deze app weergegeven met <span class="math notranslate nohighlight">\(x=x_1\)</span>  en <span class="math notranslate nohighlight">\(y=x_2\)</span>  als  <span class="math notranslate nohighlight">\(x_{1} -\frac{1}{2}x_{2}=\frac{1}{2}\)</span>.</p>
<ol class="arabic">
<li><p>In dit voorbeeld hebben we bewust de fout grens heel tolerant neer gezet en de leersnelheid hoog. Niet altijd, maar vaak, zal na de training van het netwerk een aantal punten aan de verkeerde kant van de door het netwerk berekende lijn liggen. Druk net zo lang op de  knop <strong>Lijn</strong> in het top menu tot er fout voorspelde trainingspunten zijn. In de voorspellingen lijst zijn hebben die punten een rode achtergrond. Heb je zo’n situatie gevonden druk dan een aantal malen op de <strong>Train</strong> knop en bekijk de ligging van de door het netwerk berekende lijn. Is de ligging van de lijn altijd gelijk?</p>
<ul>
<li><p>Antwoord</p>
<p>Door de grote foutmarge kan het trainen succesvol stoppen
zonder alle punten juist te hebben geclassificeerd. Er zullen
vele lijnen zijn die binnen deze foutmarge passend zijn.</p>
</li>
</ul>
</li>
<li><p>Wat valt op aan de punten die niet goed voorspeld worden?</p>
<ul>
<li><p>Antwoord</p>
<p>Deze punten liggen dicht bij de lijn  <span class="math notranslate nohighlight">\(y=2x-1\)</span>.</p>
</li>
</ul>
</li>
<li><p>Verander de fouten grens (threshold) van 0.1 eerst naar 0.01 en dan naar 0.001. Druk dan in die gevallen weer een aantal malen op de Train knop. Wat observeer je in de lijst met voorspellingen en de ligging van de door het netwerk geleverde lijn?</p>
<ul>
<li><p>Antwoord</p>
<p>Je zult zien dat bij een kleinere toegestane fout de training lijnen oplevert die beter passen en er minder punten fout worden geclassificeerd. De training duurt wel langer.</p>
</li>
</ul>
</li>
<li><p>Zet de grens van de fout weer op 0.1. Verlaag nu de leersnelheid naar 0.1. Onderzoek of dit tot een vergelijkbaar resultaat leidt.</p>
<ul>
<li><p>Antwoord</p>
<p>In dit voorbeeld heeft het verlagen van de leersnelheid veel minder effect dan het verlagen van de error threshold.</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="lijn-y-2x1-ofwel-x-1-frac-1-2-x-2-frac-1-2-met-fouten-in-de-trainingsdata">
<h3><span class="section-number">5.2.3.2. </span>Lijn <span class="math notranslate nohighlight">\(y=2x−1\)</span> ofwel <span class="math notranslate nohighlight">\(x_{1} -\frac{1}{2}x_{2}=\frac{1}{2}\)</span> met fouten in de trainingsdata<a class="headerlink" href="#lijn-y-2x1-ofwel-x-1-frac-1-2-x-2-frac-1-2-met-fouten-in-de-trainingsdata" title="Permalink to this heading">#</a></h3>
<p>In de situatie hierboven hebben we de invoer zo gemaakt dat de trainingsdata precies vertelde aan welke kant van de werkelijke lijn het punt lag, ofwel we hadden perfecte meetpunten. Nu introduceren we een fout in de metingen. Gebruiken we het voorbeeld van de aardlagen dan kan bij een meetpunt de foute aardlaag worden opgehaald. We nemen aan dat hoe dichter bij de scheidingslijn bent hoe groter de kans is dat een punt een foute uitkomst krijgt. In de simulaties wordt eerst bij toeval een punt gekozen. Dan wordt een toevalsgetal tussen 0 en 1 getrokken. Op basis van dit toevalsgetal en de afstand tot de lijn wordt bepaald of het punt een foute uitvoer krijgt.
Het netwerk weet niet dat die fouten er zijn (weet ook niets van kleurtjes die we er straks in de simulaties aan geven) en gaat proberen de lijn te schatten. Dat lukt niet altijd zoals je in het volgende onderzoekje misschien ziet (Het is en blijft tenslotte een kans proces)</p>
<ul>
<li><p>Opdrachten</p>
<p>De simulaties voor deze situatie zijn onder de knop <strong>lijn met fouten</strong> in het top menu. In het diagram krijgen foute meetpunten een groene kleur en in de lijst van voorspellingen hebben deze punten een groene kleur met zwarte achtergrond als de voorspelling door het netwerk klopt met de foute verwachting en een gele kleur met donkerrode achtergrond als de voorspelling niet in overeenstemming is met de foute verwachting.</p>
<ol class="arabic">
<li><p>Het netwerk onder knop <strong>lijn met fouten</strong> heeft standaard een threshold van 0.1
en een leersnelheid van 0.3. Je hebt bij de <strong>lijn</strong> situatie al gevonden dat de threshold de belangrijkste instelling is om de lijn zo dicht mogelijk te benaderen.
Druk nu meerdere malen op de  knop <strong>lijn met fouten</strong>.  Hoe groot is dan de fout na het stoppen van de training bij de pogingen dat het netwerk niet succesvol eindigt?</p>
<ul>
<li><p>Antwoord</p>
<p>Die ligt natuurlijk boven de 0.1, maar is vaak behoorlijk
veel groter.</p>
</li>
</ul>
</li>
<li><p>Als een training wel lukt, wordt een foutief punt dan ook altijd als een foutief punt gezien (donkerrood) of kan dat punt ook een voorspelling hebben gelijk aan de foute uitvoer waarde (zwarte achtergrond)?</p>
<ul>
<li><p>Antwoord</p>
<p>Heel vaak krijgt een foute uitvoer een vergelijkbare voorspelling. Het netwerk geeft dan eigenlijk een fout antwoord. Maar ja dat weet dat netwerk niet. Het netwerk vindt de voorspelling juist in orde.</p>
</li>
</ul>
</li>
<li><p>Zijn er altijd veel fouten nodig om het trainen te laten mislukken?</p>
<ul>
<li><p>Antwoord</p>
<p>Soms gaat het al met 1 fout mis, maar met meer foute meetpunten wordt de fout waarbij gestopt wordt groter.</p>
</li>
</ul>
</li>
<li><p>Lukt het om in een situatie waar het trainen misgaat het probleem op te lossen door meer verborgen lagen in te voeren?</p>
<ul>
<li><p>Antwoord</p>
<p>Het lukt net als bij de <strong>xor</strong> situatie vaak om met een extra verborgen laag [3] een training op een nette manier af te sluiten, meestal wordt de fout bij een mislukte training wel kleiner.</p>
</li>
</ul>
</li>
<li><p>Als het lukt om met meer lagen een succesvolle training te krijgen, hoe zit het dan met de voorspelling van de foute punten? Tip: Denk aan het xor voorbeeld.</p>
<ul>
<li><p>Antwoord</p>
<p>Het kan zo maar zijn dat alle voorspellingen van het netwerk in overeenstemming zijn met de foute invoer (alleen zwarte regels bij de voorspellingen). Waarom is dit te vergelijken met het xor voorbeeld? Bij het xor voorbeeld zijn er minimaal twee lijnen nodig om de gebieden te scheiden. Door de fouten in de trainingsdata onstaan er gebiedjes die door de inzet van meer lijnen afgebakend kunnen worden. Overdimensionering van een netwerk kan dus leiden tot een AI applicatie die alle foutieve voorbeelden goed voorspeld.</p>
</li>
</ul>
</li>
<li><p>Druk weer op de knop <strong>lijn met fouten</strong> en tel in de trainingsdata het aantal fouten. (b.v. 4) Zet lagen op [fouten +2] (b.v. [6]) leersnelheid op 0.1 en threshold op 0.01. Is dit netwerk altijd te trainen?</p>
<ul>
<li><p>Antwoord</p>
<p>Heel vaak lukt het om dit netwerk te trainen. Doordat er meerdere lijnen worden gevormd kunnen er eilandjes komen waar de fout gemeten punten in komen te liggen. Het netwerk zal dan deze fouten niet corrigeren maar juist accepteren. <strong>Klakkeloos een groot netwerk maken om het netwerk goed door een training te laten komen is geen juiste strategie om tot een betrouwbare AI applicatie
te komen.</strong></p>
</li>
</ul>
</li>
<li><p>Wat leren we van dit onderzoekje?</p>
<ul>
<li><p>Antwoord</p>
<ul class="simple">
<li><p>De kwaliteit van de trainingsdata heeft invloed op het leerproces.</p></li>
<li><p>Er is een hoge threshold waarde nodig om een netwerk een training netjes af te sluiten.</p></li>
<li><p>Ook kunnen er meerdere lagen worden ingezet.</p></li>
<li><p><strong>Het gevolg van beide strategieën is dat het netwerk ver van de echte oplossing kan zijn, waardoor de voorspellende waarde aanzienlijk afneemt en zelfs foutief kan zijn.</strong></p></li>
</ul>
<p>Best logisch eigenlijk, maar wel een groot punt van aandacht als je een AI applicatie, getraind met kwalitatief slechte trainingsdata, aan gebruikers aanbiedt</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
</section>
<section id="bias-vervolgd">
<h2><span class="section-number">5.2.4. </span>Bias vervolgd<a class="headerlink" href="#bias-vervolgd" title="Permalink to this heading">#</a></h2>
<p>We hadden beloofd terug te komen op de andere twee betekenissen van bias <strong>neiging</strong> en <strong>effect</strong>. In de onderzoekjes naar de kwaliteit van de trainingsdata heb je ervaren dat een lagere threshold waarde, de drempelwaarde van de fout in de training, vaak betere trainingsresultaten levert bij perfecte datapunten, maar dat bij fouten in de trainingsdata een hogere threshold nodig is om een training succesvol af te ronden (onder de voorwaarde dat het netwerk uit dezelfde lagen bestaat). Dit laatste levert dus wel onbetrouwbaarder uitspraken van het netwerk.  Als de trainingsdata beter zijn is het <strong>effect</strong> van de training dus beter en <strong>neigt</strong> de uitkomst meer naar de waarheid. De bias bepaalt door de trainingsdata wordt dan hoger, in dit geval een <strong>positief</strong> resultaat. De trainingsdata zijn nu de bepalende factor in de voorspellingen en bepalen nu het vooroordeel. <strong>Effect en neiging in het trainingsproces hebben dus als resultaat een vooroordeel door de AI applicatie gegenereerd door de data.</strong> 
Je hebt ook gezien dat een netwerk uitgebreid kan worden (overdimensionering) op zo’n manier dat foutieve trainingsdata ook tot een trainingsresultaat kunnen leiden dat het algoritme heel goed vindt en dus een hoge bias levert. De verhoogde bias door overdimensionering is een aanpassing aan foute trainingsdata en dus een aanpassing aan een foute aanname. Het vooroordeel heeft in dit geval dus een <strong>negatieve</strong> lading.</p>
<p>Kortom het is dus van heel groot belang om de kwaliteit van de trainingsdata vooraf te onderzoeken. Op basis van dit onderzoek moet dan het netwerk worden gevormd.</p>
<ul>
<li><p>Vragen</p>
<ol class="arabic">
<li><p>Welke betekenissen heeft het woord bias</p>
<ul>
<li><p>Antwoord</p>
<p>vooringenomenheid, vooroordeel, neiging, effect</p>
</li>
</ul>
</li>
<li><p>Welke betekenissen van bias worden beïnvloed door de keuze van het netwerk en hoe?</p>
<ul>
<li><p>Antwoord</p>
<p>neiging, effect. Als de trainingsdata perfect zijn dan kan overdimensionering een positief effect hebben op de kwaliteit van de voorspelling. Het antwoord neigt dan meer naar het juiste antwoord. Bij fouten in de trainingsdata leidt overdimensionering ook tot het voorspellen van die fouten. Dit is een negatief effect.</p>
</li>
</ul>
</li>
<li><p>Hoe kunnen vooringenomenheid en vooroordelen in een een AI applicatie terechtkomen? Geef een voorbeeld.</p>
<ul>
<li><p>Antwoord</p>
<p>Garbage in is garbage out! Bij het begeleid leren van een netwerk bestaat de trainingsdata altijd uit vooroordelen. De trainer voorziet het antwoord bij een trainingspunt. Als in de trainingsdata niet alle mogelijke vooroordelen aanwezig zijn, kunnen deze vooroordelen ook niet in het antwoord van de AI applicatie voorkomen. B.v. Als de trainingsdata alleen datapunten bevat van lachende mensen (gelukkig) en huilende mensen (ongelukkig) dan kan het netwerk niet anders dan één van deze twee mogelijkheden als antwoord geven.</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="lijnen-vervolgd">
<h2><span class="section-number">5.2.5. </span>Lijnen vervolgd<a class="headerlink" href="#lijnen-vervolgd" title="Permalink to this heading">#</a></h2>
<p>In het vorige hoofdstuk  <a class="reference external" href="https://www.notion.so/N-1-Hoe-leert-een-neuraal-netwerk-f6445c4d563a48dd867c16d63fc55254">Hoe leert een neuraal netwerk</a> hebben we één grens kunnen bepalen tussen een verzameling punten met behulp van één perceptron. Bij het <strong>xor</strong> (exclusieve-of) voorbeeld hierboven waren er twee lijnen nodig om deze logische schakeling te leren. In dit deel bekijken we puntenverzamelingen die op basis van twee lijnen zijn ingedeeld. We beschouwen twee situaties. Een situatie waarin we het netwerk willen leren een gebied te vinden waar punten zowel boven lijn 1 en boven lijn 2 ligt (<strong>boven 2 lijnen</strong>). Een tweede situatie waarin we het netwerk willen leren een gebied te vinden waar punten zowel boven lijn 1 en boven lijn 2 ligt of zowel onder lijn 1 en onder lijn 2 ligt (<strong>tussen 2 lijnen</strong>). De vraag waaraan we hier aandacht besteden is: Hoe ziet het meest minimale netwerk eruit om elk van de twee situaties op te lossen.</p>
<p><img alt="boven2lijnen.png" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ab01e989-cf71-4283-b305-b3c936cf6534/boven2lijnen.png" /></p>
<p><img alt="figuur 6: Twee lijnen: De blauwe punten moeten worden gescheiden van de rode punten. De bovenste figuur is de situatie boven 2 lijnen, de onderste figuur de situatie tussen twee lijnen. De groene lijnen zijn de werkelijke lijnen. De gestippelde blauwe de door training verkregen lijnen." src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b70e0eed-a36a-43b5-bdbe-5c49e53e8add/tussen2lijnen.png" /></p>
<p>figuur 6: Twee lijnen: De blauwe punten moeten worden gescheiden van de rode punten. De bovenste figuur is de situatie <strong>boven 2 lijnen</strong>, de onderste figuur de situatie <strong>tussen twee lijnen</strong>. De groene lijnen zijn de werkelijke lijnen. De gestippelde blauwe de door training verkregen lijnen.</p>
<ul>
<li><p>Opdrachten</p>
<p><strong>Boven twee lijnen:</strong></p>
<ol class="arabic">
<li><p>Waarom volstaat hier het netwerk hieronder met één verborgen laag met twee knopen en één uitvoer knoop?</p>
<p><img alt="netwerkBovenline.svg" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cb7f4eab-acfe-44d3-ad15-78b80da494b2/netwerkBovenline.svg" /></p>
<ul>
<li><p>Antwoord</p>
<p>Het probleem kun je vergelijken met twee ja/nee (1/-1) schakelaars. Één schakelaar voor lijn 1 en één schakelaar voor lijn 2. Je weet al dat één perceptron één grens kan leren of wel één schakelaar kan. Als een punt boven lijn 1 ligt moet het antwoord van die perceptron ja (1) evenzo als dat punt boven lijn 1 ligt moet het antwoord van die perceptron ook ja (1) zijn. In andere gevallen moet één van de perceptrons niet het antwoord ja <a class="reference external" href="http://geven.De">geven.De</a> verborgen laag bevat nu die twee schakelaars en deze vormen de input voor de uitvoer knoop net als de <strong>en</strong> situatie bij de logische schakelingen.</p>
</li>
</ul>
</li>
<li><p>Open weer de <a class="reference external" href="https://www.johnval.nl/school/informatica/Artificial_Intelligence/SLO-apps/NNinputAll.htm">applet</a>. Druk in de applet op de knop <strong>tussen 2 lijnen</strong> tot er een situatie is waarbij de training is gelukt.</p>
<ol class="arabic">
<li><p>Overtuig jezelf dat inderdaad het netwerk uit de vorige vraag wordt gebruikt</p>
<ul>
<li><p>Antwoord</p>
<p>Druk op het tabblad <strong>netwerk</strong>.</p>
</li>
</ul>
</li>
<li><p>Bekijk de twee <span class="math notranslate nohighlight">\(x_{1},x_{2}\)</span> diagrammen. In het bovenste diagram zie je de toestand van de verborgen laag. Waarom?</p>
<ul>
<li><p>Antwoord</p>
<p>Je ziet alle ingevoerde punten en die komen uit de invoer. Ook zie je de twee blauw gestippelde lijnen. Dit zijn de door training gevonden lijnen.</p>
</li>
</ul>
</li>
<li><p>In het onderste diagram zie je de toestand van de perceptron uit de uitvoer laag. Je ziet daar vier groepjes punten ongeveer in een vierkant en een lijn. Waar komen die vandaan?</p>
<ul>
<li><p>Antwoord</p>
<p>Je ziet een groepje dicht bij het punt (1,1), een groepje dicht bij het punt (-1,1),  en groepje dicht bij het punt (1,-1) en een groepje dicht bij het punt (-1,-1). Het blauwe groepje hoort bij de punten die boven de twee lijnen liggen. Schuin tegenover dit blauwe groepje bevindt zich een groepje met punten die onder beide lijnen liggen. De andere twee groepjes bevatten punten die boven één van de lijnen liggen en onder de andere.
Dat de punten zich in die hoeken verzamelen komt door de keuze van de tangenshyperbolicus als activeringsfunctie. Deze functie geeft waarden tussen -1 en 1. Hoe groter de som hoe dichter de waarde bij 1 (of -1 bij negatieve waarde) komt te liggen.
De blauwe lijn is de grens die door training voor de uitvoer perceptron is gevonden. Deze hoort natuurlijk netjes de blauwe van de rode punten te scheiden.</p>
</li>
</ul>
</li>
<li><p>Druk bij een geslaagde training nog een paar keer op <strong>train</strong> en bekijk de ligging van het groepje blauwe cellen. Ligt dit groepje altijd in de zelfde hoek?
Geef een verklaring.</p>
<ul>
<li><p>Antwoord</p>
<p>Het groepje ligt zeker niet altijd in de zelfde hoek, sterker nog iedere hoek kan het zijn. Verklaring: De gewichten worden bij start van de training willekeurig gekozen. Welke perceptron in de verborgen laag lijn 1 gaat benaderen ligt niet vast. Ook ligt de richting van de lijnen niet vast, daarom kan de input voor de uitvoer perceptron precies andersom zijn. Dus 2x2 is 4 mogelijkheden.</p>
</li>
</ul>
</li>
<li><p>Worden altijd alle trainingsdata goed voorspeld?</p>
<ul>
<li><p>Antwoord</p>
<p>Niet noodzakelijk, de threshold kan soms gehaald worden met enkele fouten afhankelijk van de waarde van de drempelwaarde (threshold).</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>Druk op de knop <strong>boven 2 lijnen</strong> tot er een situatie is waarbij de training mislukt.</p>
<ol class="arabic">
<li><p>Hoeveel punten zijn er niet goed voorspeld?</p>
<ul>
<li><p>Antwoord</p>
<p>Druk op het tabblad <strong>voorspelling</strong>. Er moeten zeker fouten zijn.</p>
</li>
</ul>
</li>
<li><p>Druk een paar keer op <strong>train</strong>. Lukt het nu wel?</p>
<ul>
<li><p>Antwoord</p>
<p>Misschien wel, misschien niet.</p>
</li>
</ul>
</li>
<li><p>Probeer door andere instellingen (lagen, leersnelheid, …) te kiezen trainingen wel te
laten slagen? Wat helpt het best.</p>
<ul>
<li><p>Antwoord</p>
<p>De leersnelheid verlagen is het meest succesvol, maar wel meer tijdrovend. Meer knopen in de verborgen laag helpt soms ook. Een combinatie van beiden heeft nog meer kans op succes. Denk aan de discussie over bias en het trainen van de logische schakeling <strong>excusieve-of</strong>.</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<p><strong>Tussen twee lijnen:</strong></p>
<ol class="arabic">
<li><p>Waarom volstaat hier een netwerk met één verborgen laag met twee knopen en één uitvoer knoop niet?</p>
<ul>
<li><p>Antwoord</p>
<p>De toestanden van de twee schakelaars (de geleerde lijnen) moeten in (-1,-1) (beiden onder) of (1,1) beide boven staan. Dit komt overeen met de <strong>exclusieve-of</strong> logische schakeling. Daar moest een extra verborgen laag worden toegevoegd. Dit moet dus ook in dit geval worden toegevoegd. [2,2] is dan het minimale netwerk, dat nodig is.</p>
</li>
</ul>
</li>
<li><p>Open weer de <a class="reference external" href="https://www.johnval.nl/school/informatica/Artificial_Intelligence/SLO-apps/NNinputAll.htm">applet</a>. Druk op de knop <strong>tussen 2 lijnen</strong> tot er een situatie is waarbij de training is gelukt.</p>
<ol class="arabic">
<li><p>Overtuig jezelf dat inderdaad het netwerk uit de vorige vraag wordt gebruikt.</p>
<ul>
<li><p>Antwoord</p>
<p>Druk op het tabblad <strong>netwerk</strong>.</p>
</li>
</ul>
</li>
<li><p>Je ziet nu drie <span class="math notranslate nohighlight">\(x_{1},x_{2}\)</span> diagrammen. Welke hoort bij welke laag.</p>
<ul>
<li><p>Antwoord</p>
<p>bovenste-&gt;eerste verborgen laag.
middelste-&gt;tweede verborgen laag.
onderste-&gt;uitvoer laag.</p>
</li>
</ul>
</li>
<li><p>Vergelijk de diagrammen met de logische schakeling <strong>exclusieve-of (xor)</strong>. Wat neem je waar?</p>
<ul>
<li><p>Antwoord</p>
<p>De tweede verborgen laag komt ongeveer overeen met de eerste verborgen laag van de <strong>xor</strong> trainer.</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</section>
<section id="eigen-quickdraw-vervolgd">
<h2><span class="section-number">5.2.6. </span>Eigen Quickdraw vervolgd<a class="headerlink" href="#eigen-quickdraw-vervolgd" title="Permalink to this heading">#</a></h2>
<p>In het hoofdstuk <a class="reference external" href="https://www.notion.so/3-6-Eigen-Quickdraw-4104692fbbbd4462b95b891aa08a51ee">technieken</a> heb je gespeeld  met een eigen Quickdraw applicatie die je verschillende dieren hebt leren herkennen. Hier stellen we de vraag: Hoe moet een minimaal netwerk er uitzien om 2,3,4, … n dieren te leren herkennen. In de eigen Quickdraw applicatie is de input een plaatje bestaande uit zwartwit beeldpunten (pixels). Stel er zijn <span class="math notranslate nohighlight">\(n\)</span> beeldpunten.</p>
<ul>
<li><p>Opdrachten</p>
<ol class="arabic">
<li><p>Hoeveel inputs zijn er in het netwerk?</p>
<ul>
<li><p>Antwoord</p>
<p>In het eigen Quickdraw hoofdstuk heb je al gezien dat ieder beeldpunt één input is. Samen met de bias input zijn er dus <span class="math notranslate nohighlight">\(n+1\)</span> inputs.</p>
</li>
</ul>
</li>
<li><p>Bewering: Twee dieren leren herkennen is hetzelfde probleem als het leren van de schakelingen <strong>of</strong> en <strong>en</strong> en het leren van één scheidingslijn in een verzameling punten.
Is deze bewering waar of niet waar? Motiveer je antwoord.</p>
<ul>
<li><p>Antwoord</p>
<p>Deze bewering is waar. De dimensie van de input in een knoop na de input is <span class="math notranslate nohighlight">\(n+1\)</span>. De input bij de schakeling <strong>of</strong> en <strong>en</strong> en de lijn bestond uit een x en een y coördinaat en de 1 voor de bias input dus de dimensie is 3 voor de knoop direct na de input. Een knoop met een input van dimensie 3 moet een rechte lijn leren. Een rechte lijn is een twee imensionale structuur. Een  knoop  met dimensie <span class="math notranslate nohighlight">\(n + 1\)</span> als input moet een <span class="math notranslate nohighlight">\(n\)</span> dimensionale ‘lineaire’ structuur leren:
<span class="math notranslate nohighlight">\(w_{0}+w_{1}x_{1}+w_{2}x_{2}+ \cdots + w_{n}x_{n}=0\)</span></p>
<p>Een hoger dimensionale lineaire structuur van deze vorm noemt men een <strong>hypervlak</strong>  van dimensie <strong>n</strong>. Een plaatje als geheel ligt dan boven of onder het te leren hypervlak net als de punten in de voorbeelden van de schakelingen. Neem bijvoorbeeld het leren herkennen van alleen hond of kat. Een kat is dan geen hond. Je hebt dan slechts te leren hond of geen hond. Alle honden plaatjes komen dan aan één kant van het te leren hypervlak, de geen hondjes aan de andere kant.</p>
</li>
</ul>
</li>
<li><p>Wat is het minimale netwerk om twee dieren te leren herkennen.</p>
<ul>
<li><p>Antwoord</p>
<p>Een enkele output perceptron zou volstaan. Echter  de definitie van de output in de plaatjes bij de eigen Quickdraw app is met de naam van het dier. Ieder dier krijgt dan zijn eigen output perceptron. Die perceptrons leren dan direct van de input. Er is geen verborgen laag nodig.</p>
<p>Gebruik <a class="reference external" href="https://www.johnval.nl/school/informatica/SLO_Artificial_Intelligence/SLO-apps/Emoticons/index.html">deze versie</a> van de eigen Quickdraw app en train het netwerk met slechts twee diersoorten en geef Deep layers de waarde [].
Overtuig jezelf dat het getrainde netwerk een goede scheiding geeft tussen de twee diersoorten.</p>
</li>
</ul>
</li>
<li><p>Bewering: Drie dieren leren herkennen (b.v.: kat , hond krokodil ) is hetzelfde probleem als het leren van de het leren van de situatie <strong>boven twee lijnen</strong> bij een verzameling punten.
Is deze bewering waar of niet waar? Motiveer je antwoord.</p>
<ul>
<li><p>Antwoord</p>
<p>Deze bewering is niet helemaal waar, maar komt wel in de buurt. Stel je hebt drie dieren: hond,kat,krokodil. Je kunt dan zeggen dat een krokodil geen kat is <strong>en</strong> ook geen hond. Als we dan alleen naar kat/geen kat en hond/geen hond kijken dan is het krokodil zijn gelijk aan de situatie geen kat en geen hond ofwel gelijk aan de situatie <strong>boven twee lijnen</strong>. Er zijn dus twee knopen in de verborgen laag nodig. Het is nu wel noodzakelijk dat ieder dier zijn eigen output knoop heeft.</p>
</li>
</ul>
</li>
<li><p>Welke situaties in onderstaande figuur horen bij respectievelijk kat, hond en krokodil?</p>
<p><img alt="kat_hond_krokodil.svg" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/933cd601-cdd1-4e79-b0c5-a94877ced5e6/kat_hond_krokodil.svg" /></p>
<ul>
<li><p>Antwoord</p>
<p><img alt="kat_hond_krokodil_antwoord.svg" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dcd09048-0a4c-452b-adf7-f6b2f6d1d8d6/kat_hond_krokodil_antwoord.svg" /></p>
</li>
</ul>
</li>
<li><p>Gebruik weer <a class="reference external" href="https://www.johnval.nl/school/informatica/SLO_Artificial_Intelligence/SLO-apps/Emoticons/index.html">deze versie</a> van van de eigen Quickdraw app en train het netwerk met drie diersoorten en geef Deep layers de waarde [2].
Overtuig jezelf dat het getrainde netwerk een goede scheiding geeft tussen de drie diersoorten.</p></li>
<li><p>Hoewel we bij vraag 5 in één vakje <strong>onmogelijk</strong> hebben ingevuld kan het netwerk met vier knopen (=diersoorten) in de output laag deze toch gebruiken om dit onlogische vak te gebruiken om de vierde diersoort in te plaatsen. Overtuig jezelf dat je de situatie Deep layers: [2]  ook kan gebruiken om vier diersoorten te onderscheiden.</p></li>
<li><p>Vermoeden: Voor <span class="math notranslate nohighlight">\(n\)</span> diersoorten heb je in de tweede laag minimaal <span class="math notranslate nohighlight">\(^{2}\log{n}\)</span> afgerond naar boven knopen in de tweede laag nodig hebt.
( <span class="math notranslate nohighlight">\(^{2}\log{2}=1\)</span>, <span class="math notranslate nohighlight">\(^{2}\log{3}=1.58 \rightarrow 2\)</span>,  <span class="math notranslate nohighlight">\(^{2}\log{4}=2, \cdots\)</span>,  <span class="math notranslate nohighlight">\(^{2}\log{8}=3, \cdots\)</span>)
Beargumenteer dat dit vermoeden waar kan zijn.</p>
<ul>
<li><p>Antwoord</p>
<p>Een knoop is een schakelaar, met één knoop heb je twee standen voor de schakelaar. Voeg je een knoop ofwel schakelaar toe dan zijn er <span class="math notranslate nohighlight">\(2 \cdot 2 = 4\)</span> standen. Een derde knoop geeft dan de mogelijkheid van <span class="math notranslate nohighlight">\(2 \cdot 2 \cdot 2= 8\)</span> standen, enzovoorts. Om 8 diersoorten te onderscheiden zou Deep layers: [3]  mogelijk moeten zijn.</p>
</li>
</ul>
</li>
<li><p>Is dit minimum aantal ook altijd voldoende?</p>
<ul>
<li><p>Antwoord</p>
<p>Het minimum aantal is voldoende als de scheiding tussen de diersoorten altijd lineair is. Is dit niet het geval dan zijn er voor de scheiding meerdere hypervlakken nodig. Vaak worden er dan meerdere verborgen lagen ingezet om het probleem op te lossen.</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>Je hebt nu kennis gemaakt met neurale netwerken, hoe ze leren, dat bias een probleem is en dat ook de samenstelling van netwerken overdacht moet worden. Meer verdieping in de theorie willen we je hier niet meer bieden. Misschien heeft deze cursus je nu warm gemaakt voor een studie kunstmatige intelligentie. Wil je geen volledige studie gaan volgen maar wil je toch meer kennis dan zijn er ook nog vele gratis cursussen via universiteiten te volgen. Kijk op <a class="reference external" href="https://www.coursera.org/search?query=artificial%20intelligence&amp;">Coursera</a> voor een uitgebreid aanbod. In de volgende paragraaf laten we je een hele AI applicatie bouwen. Je ziet dan het hele proces van model naar trainen naar agent op een mobiel.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./neurale-netwerken"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="hoe-leert-neuraal-netwerk.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">vorige</p>
        <p class="prev-next-title"><span class="section-number">5.1. </span>N.1. Hoe leert een neuraal netwerk</p>
      </div>
    </a>
    <a class="right-next"
       href="tensorflow-app.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">volgende</p>
        <p class="prev-next-title"><span class="section-number">6. </span>N.3 Tensorflow app</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Inhoud
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simpele-logische-schakelingen">5.2.1. Simpele logische schakelingen</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dierenliefhebber">5.2.1.1. Dierenliefhebber</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hotel-de-botel-van-dieren">5.2.1.2. Hotel de botel van dieren</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verantwoordelijke-voortplanting">5.2.1.3. Verantwoordelijke voortplanting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">5.2.2. Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#veel-gegevens-toch-simpel-lijnen">5.2.3. Veel gegevens toch simpel: lijnen</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lijn-y-2x-1-ofwel-x-frac-1-2-y-frac-1-2">5.2.3.1. Lijn <span class="math notranslate nohighlight">\(y=2x-1\)</span> ofwel <span class="math notranslate nohighlight">\(x - \frac{1}{2}y= \frac{1}{2}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lijn-y-2x1-ofwel-x-1-frac-1-2-x-2-frac-1-2-met-fouten-in-de-trainingsdata">5.2.3.2. Lijn <span class="math notranslate nohighlight">\(y=2x−1\)</span> ofwel <span class="math notranslate nohighlight">\(x_{1} -\frac{1}{2}x_{2}=\frac{1}{2}\)</span> met fouten in de trainingsdata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-vervolgd">5.2.4. Bias vervolgd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lijnen-vervolgd">5.2.5. Lijnen vervolgd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigen-quickdraw-vervolgd">5.2.6. Eigen Quickdraw vervolgd</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Door cc-team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright SLO 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>